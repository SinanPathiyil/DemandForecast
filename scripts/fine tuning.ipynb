{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46989318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec924ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, product_name):\n",
    "    \"\"\"\n",
    "    Preprocess data for a single product:\n",
    "    - Filters product-specific data\n",
    "    - Sorts by year and week number\n",
    "    - Normalizes TotalQuantity\n",
    "    \"\"\"\n",
    "    # Filter data for selected product\n",
    "    product_df = df[df['Product_Name'] == product_name].copy()\n",
    "\n",
    "    # Sort by Year THEN WeekNumber\n",
    "    product_df = product_df.sort_values(['Year', 'Week_Number']).reset_index(drop=True)\n",
    "\n",
    "    # Extract TotalQuantity as numpy array\n",
    "    data = product_df['Total_Quantity'].values.reshape(-1, 1)\n",
    "\n",
    "    # Handle missing values\n",
    "    data = np.nan_to_num(data)\n",
    "\n",
    "    # Normalize between 0 and 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    return scaled_data, scaler, product_df, data\n",
    "\n",
    "\n",
    "def create_dataset(scaled_data, time_steps=12):\n",
    "    \"\"\"\n",
    "    Creates sequences of data for LSTM input:\n",
    "    - X: past time steps\n",
    "    - y: next step prediction target\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - time_steps):\n",
    "        X.append(scaled_data[i:(i + time_steps), 0])  # take past demand\n",
    "        y.append(scaled_data[i + time_steps, 0])      # predict next demand\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d54d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"../saved models/good.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42076321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: freeze layers\n",
    "\n",
    "#for layer in model.layers[:-2]:  # freeze all but the last 2 layers\n",
    "#    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d85ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",  # adjust if using something else\n",
    "    metrics=[\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7280e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_dataset\u001b[49m,               \u001b[38;5;66;03m# your training dataset\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,                    \u001b[38;5;66;03m# fewer epochs for fine-tuning\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_dataset  \u001b[38;5;66;03m# optional validation data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,               # your training dataset\n",
    "    epochs=5,                    # fewer epochs for fine-tuning\n",
    "    validation_data=val_dataset  # optional validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15bcec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=[early_stop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
